{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" src=\"https://d3rt91u8ecpt22.cloudfront.net/assets/learn/logo-exts-0e71782f000e506b332ae30887d6a959dd3a13bcc0d6fb6bb7797c4f1100a537.svg\" alt=\"drawing\" width=\"100\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project - Liberty Mutual Group Fire Peril Loss Cost\n",
    "\n",
    "\n",
    "https://www.kaggle.com/c/liberty-mutual-fire-peril\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 6. Model Appraisal: Classification <a name=\"6\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start the regression models, we first introduce measures that will help us understand the underlying problem of the challenge at hand and how quality and validity of the models can be evaluated.\n",
    "\n",
    "In this section, we will look at classification metrics. Although this challenge is inherently a regression problem, we also will look at the challenge as a classification problem as the data could be transformed to classify target variables `> 0` as loss events and target values `== 0` as non-events. Hencce, this chapter introduces metrics to measure classification performance.\n",
    "\n",
    "Firstly, we start with the simple accuracy score, which simply summarises the number of times we got the right prediction. Secondly, we introduce the confusion matrix and the measures derived from it. Lastly, we will elaborate more on Type I and Type II erros and their importance in the context of this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Simple Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For illustrative purposes, we take the short and compressed version of the training set. This increases training speed as it is only for iullstrative purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# separate features from target\n",
    "d = df_train_dict['short_comp']\n",
    "X = d.drop(['LogTarget', 'isClaim'], axis=1)\n",
    "X = pd.get_dummies(X, columns=intersection(nominals,X.columns), prefix=intersection(nominals,X.columns))\n",
    "y = d['isClaim']\n",
    "\n",
    "# creat train and test sets\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "    X,\n",
    "    y, \n",
    "    train_size=0.8, \n",
    "    random_state=0,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "logreg = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "logreg.fit(X_tr, y_tr)\n",
    "y_pred = logreg.predict(X_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Baseline Accuracy: Most Frequent Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the baseline accuracy we establish an idea of how accurate a purely random model will be in predicting the most frequent class. This gives us a baseline of what we can expect at the very least of a model under scrutiny."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1665\n",
       "1    166 \n",
       "Name: isClaim, dtype: int64"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_te.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have approximately ten times the amout of non-events as compared to the loss events. In reality, this imbalance is much larger. However, since we work in this example with a heavily resampled datase, where most of the majority class events `(loss == 0)` has been downsampled to a majority-to-minority-class ratio of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of positive values: 0.0907\n",
      "Percentage of negative values: 0.9093\n",
      "-------------------------------------\n",
      "Most Frequent Baseline:        0.9093\n"
     ]
    }
   ],
   "source": [
    "pos_pct = y_te.mean()\n",
    "neg_pct = 1 - y_te.mean()\n",
    "baseline = max(pos_pct, neg_pct)\n",
    "\n",
    "print('Percentage of positive values:', round(pos_pct,4))\n",
    "print('Percentage of negative values:', round(neg_pct,4))\n",
    "print('-------------------------------------')\n",
    "print('Most Frequent Baseline:       ', round(baseline,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Given the imbalance of the dataset, we would be 90% accurate just by randomly guessing and taking under consideration  the underlying probability distribution of true and false values.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3. Simple Accuracy Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score\n",
    "print('Accuracy score:', round(balanced_accuracy_score(y_te, np.zeros_like(y_te)),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1665\n",
       "1    166 \n",
       "Name: isClaim, dtype: int64"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_te.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.9093\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy score:', round(accuracy_score(y_te, np.zeros_like(y_te)),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the first 25 values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True: [0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0]\n",
      "Pred: [0 1 1 1 0 0 1 0 1 0 0 0 1 1 0 0 1 1 0 0 0 1 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "print('True:', y_te.values[0:25])\n",
    "print('Pred:', y_pred[0:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above output, we observe the following:\n",
    "- We predicted three true positive value correctly (True positives)\n",
    "- We predicted 13 true negative values correcly (True negatives)\n",
    "\n",
    "\n",
    "- **We predicted one true positive values incorrecly as negative**. (False negatives)\n",
    "- **We predicted eight true positive values incorrecly as positive**. (False positives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3. The Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's apply the above analysis to the entire set of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Negative</th>\n",
       "      <th>Predicted Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True Negative</th>\n",
       "      <td>1149</td>\n",
       "      <td>516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True Positive</th>\n",
       "      <td>76</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Predicted Negative  Predicted Positive\n",
       "True Negative  1149                516               \n",
       "True Positive  76                  90                "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "def confusion_matrix_df(y_test, y_pred):\n",
    "    cm = pd.DataFrame(confusion_matrix(y_te, y_pred), \n",
    "                      index=['True Negative', 'True Positive'], \n",
    "                      columns=['Predicted Negative', 'Predicted Positive'])\n",
    "    return cm\n",
    "confusion_matrix_df(y_te, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Basic terminology**\n",
    "\n",
    "- <font color='green'>**True Positives (TP):** we correctly predicted the loss event 90 times\n",
    "- **True Negatives (TN):** we correctly predicted no loss event 1149 times</font>\n",
    "\n",
    "\n",
    "- <font color='red'>**False Positives (FP):** we incorrectly predicted that a loss event occured (a \"Type I error\") 76 times\n",
    "- **False Negatives (FN):** we incorrectly predicted that a no loss event occured (a \"Type II error\") **516 times**</font>\n",
    "\n",
    "**Note that Type II errors are the ones we should be minimise in an insurence context. False negatives can take an insurance company into financial distress.** Incorrecly predicing no loss occuring will cause the solvency reserves to be lower than actually needed. Thereforen, in the event of an unprediced loss, the insurance company may not have adequate reserves to cover their losses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4. Metrics from a confusion matrix\n",
    "\n",
    "We will now look at several metrics that can be derived from the confusion matrix.\n",
    "\n",
    "- Classification Accuracy\n",
    "- Classification Error\n",
    "- Sensitivity\n",
    "- Specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save confusion matrix and slice into four pieces\n",
    "confusion = confusion_matrix(y_te, y_pred)\n",
    "TP = confusion[1, 1]\n",
    "TN = confusion[0, 0]\n",
    "FP = confusion[0, 1]\n",
    "FN = confusion[1, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy:  0.6766794101583834\n",
      "Accuracy Score:           0.6766794101583834\n"
     ]
    }
   ],
   "source": [
    "print('Classification Accuracy: ', (TP + TN) / (TP + TN + FP + FN))\n",
    "print('Accuracy Score:          ', accuracy_score(y_te, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Error (Misclassification Rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Error: 0.3233205898416166\n",
      "1 - Accuracy Score:   0.3233205898416166\n"
     ]
    }
   ],
   "source": [
    "print('Classification Error:', (FP + FN) / (TP + TN + FP + FN))\n",
    "print('1 - Accuracy Score:  ', 1 - accuracy_score(y_te, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sensitivity (Recall, True Positive Rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **When the actual value is positive, how often is the prediction correct?**\n",
    "\n",
    "> It is the ratio of the number of true positives to the sum of the true positive row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity:  0.5421686746987951\n"
     ]
    }
   ],
   "source": [
    "print('Sensitivity: ', TP / (TP + FN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, it means that we are approximately 54% accurate that we have predicted a true loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specificity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ** When the actual value is negative, how often is the prediction correct?**\n",
    "\n",
    "> It is the ratio of true negatives to the total of the true negative row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specificity:  0.69009009009009\n"
     ]
    }
   ],
   "source": [
    "print('Specificity: ', TN / (TN + FP))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means, that we are approximately 69% accurate in predicting true non-events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ** When a positive value is predicted, how often is the prediction correct?\n",
    "\n",
    "> It is ratio of true positives to predicted positives column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.1485148514851485\n"
     ]
    }
   ],
   "source": [
    "print('Precision:', TP / (TP + FP))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means, that we can be 15% confident that are prediction of a loss actually is a loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5. Objective in classification\n",
    "\n",
    "We can now see that we have to\n",
    "\n",
    "- Maximize Sensitivity\n",
    "- Maximize Specificity\n",
    "- Maximize Precision\n",
    "\n",
    "However, the maximization of all three measures at the same time may be difficult. Hence, the objective will be to balance out  all of the three measures. **Also, specificity (true negative rate) may not be as important as sensitivity (true positive rate) in this context**\n",
    "\n",
    "><font color='red'>**Hence, in the context of this insurance project, optimise for sensitivity at the expense of lowering specificity.**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6. Adjusting the classification threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_metrics(y_te, y_pred):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    confusion = confusion_matrix(y_te, y_pred)\n",
    "    TP = confusion[1, 1]\n",
    "    TN = confusion[0, 0]\n",
    "    FP = confusion[0, 1]\n",
    "    FN = confusion[1, 0]\n",
    "    \n",
    "    return pd.DataFrame({'Accuracy Score': (TP + TN) / (TP + TN + FP + FN), \n",
    "                         'Error Rate': (FP + FN) / (TP + TN + FP + FN), \n",
    "                         'Sensitivity': (TP) / (TP + FN),\n",
    "                         'Specificity': (TN) / (TN + FN),\n",
    "                         'Precision': (TP) / (TP + FP)\n",
    "                        }, index=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Some classifications:              ', logreg.predict(X_te)[5:10])\n",
    "print('Their probabilities of being true: ', logreg.predict_proba(X_te)[5:10, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the predicted probabilities for class 1\n",
    "y_pred_prob = logreg.predict_proba(X_te)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allow plots to appear in the notebook\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# histogram of predicted probabilities\n",
    "plt.hist(y_pred_prob, bins=200)\n",
    "plt.xlim(0, 1)\n",
    "plt.title('Histogram of predicted probabilities')\n",
    "plt.xlabel('Predicted probability of a loss event')\n",
    "plt.ylabel('Frequency');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decrease the threshold** for predicting loss events in order to **increase the sensitivity** of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import binarize\n",
    "\n",
    "res = pd.DataFrame()\n",
    "\n",
    "for t in np.arange(0,1,0.01):\n",
    "    y_pred_adj = binarize([y_pred_prob], round(t,2))[0]\n",
    "    confusion = confusion_metrics(y_te, y_pred_adj)\n",
    "    confusion.insert(loc=0, column='Probability Threshold', value=[round(t,2)])\n",
    "    res = confusion.append(res).sort_values(by='Probability Threshold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(res['Probability Threshold'], res['Sensitivity'], color='b', label='Sensitivity (Recall, True Positive Rate)')\n",
    "plt.plot(res['Probability Threshold'], res['Specificity'], color='g', label='Specificity')\n",
    "plt.plot(res['Probability Threshold'], res['Precision'], color='r', label='Precision')\n",
    "plt.xlabel('Probability Threshold')\n",
    "plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:**\n",
    "\n",
    "- Threshold of 0.5 is used by default (for binary problems) to convert predicted probabilities into class predictions\n",
    "- Threshold can be adjusted to increase sensitivity or specificity\n",
    "- Sensitivity and specificity have an inverse relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.7. Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_te, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.8 Weighted Gini-Loss\n",
    "\n",
    "In order to compare model performance with the official Kaggle leaderboard, we calculate the normalized, weighted Gini coefficient.\n",
    "\n",
    "According to the Kaggle website, the normalized, weighted Gini coefficient is calculated as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_gini(act,pred,weight): \n",
    "    df = pd.DataFrame({\"act\":act,\"pred\":pred,\"weight\":weight}) \n",
    "    df = df.sort_values('pred',ascending=False) \n",
    "    df[\"random\"] = (df.weight / df.weight.sum()).cumsum() \n",
    "    total_pos = (df.act * df.weight).sum() \n",
    "    df[\"cumposfound\"] = (df.act * df.weight).cumsum() \n",
    "    df[\"lorentz\"] = df.cumposfound / total_pos \n",
    "    n = df.shape[0] \n",
    "\n",
    "    gini = sum(df.lorentz[1:].values * (df.random[:-1])) - sum(df.lorentz[:-1].values * (df.random[1:])) \n",
    "    return gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_weighted_gini(act,pred,weight):\n",
    "    return weighted_gini(act,pred,weight) / weighted_gini(act,act,weight)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
